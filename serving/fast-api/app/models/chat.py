# Copyright 2025 NVIDIA CORPORATION & AFFILIATES
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from typing import List, Optional, Union, Dict, Any, Literal
from pydantic import BaseModel, Field
import time


class ChatMessage(BaseModel):
    role: Literal["system", "user", "assistant"] = Field(
        ..., description="The role of the message author"
    )
    content: str = Field(..., description="The contents of the message")


class ChatCompletionRequest(BaseModel):
    model: str = Field(..., description="ID of the model to use")
    messages: List[ChatMessage] = Field(
        ..., description="A list of messages comprising the conversation so far"
    )
    max_tokens: Optional[int] = Field(
        None, description="The maximum number of tokens to generate"
    )
    temperature: Optional[float] = Field(
        0.0, description="What sampling temperature to use, between 0 and 2"
    )
    top_p: Optional[float] = Field(
        None, description="Nucleus sampling parameter"
    )
    top_k: Optional[int] = Field(
        None, description="Top-k sampling parameter"
    )
    stream: Optional[bool] = Field(
        False, description="Whether to stream back partial progress"
    )
    stop: Optional[Union[str, List[str]]] = Field(
        None, description="Up to 4 sequences where the API will stop generating further tokens"
    )
    # Dream-specific parameters
    steps: Optional[int] = Field(
        128, description="Number of diffusion steps"
    )
    alg: Optional[str] = Field(
        "entropy", description="Algorithm to use for token selection"
    )
    alg_temp: Optional[float] = Field(
        0.1, description="Temperature for algorithm"
    )
    block_length: Optional[int] = Field(
        32, description="Block length for generation"
    )


class PromptTokensDetails(BaseModel):
    cached_tokens: int = Field(0, description="Number of cached tokens")
    audio_tokens: int = Field(0, description="Number of audio tokens")


class CompletionTokensDetails(BaseModel):
    reasoning_tokens: int = Field(0, description="Number of reasoning tokens")
    audio_tokens: int = Field(0, description="Number of audio tokens")
    accepted_prediction_tokens: int = Field(0, description="Number of accepted prediction tokens")
    rejected_prediction_tokens: int = Field(0, description="Number of rejected prediction tokens")


class Usage(BaseModel):
    prompt_tokens: int = Field(..., description="Number of tokens in the prompt")
    completion_tokens: int = Field(..., description="Number of tokens in the completion")
    total_tokens: int = Field(..., description="Total number of tokens")
    prompt_tokens_details: PromptTokensDetails = Field(
        default_factory=PromptTokensDetails,
        description="Breakdown of tokens in the prompt"
    )
    completion_tokens_details: CompletionTokensDetails = Field(
        default_factory=CompletionTokensDetails,
        description="Breakdown of tokens in the completion"
    )


class ChatCompletionChoice(BaseModel):
    index: int = Field(..., description="The index of the choice in the list of choices")
    message: ChatMessage = Field(..., description="A chat completion message generated by the model")
    logprobs: Optional[Dict[str, Any]] = Field(
        None, description="Log probability information for the choice"
    )
    finish_reason: Literal["stop", "length", "content_filter", "tool_calls", "function_call"] = Field(
        ..., description="The reason the model stopped generating tokens"
    )


class ChatCompletionResponse(BaseModel):
    id: str = Field(..., description="A unique identifier for the chat completion")
    object: Literal["chat.completion"] = Field(
        "chat.completion", description="The object type, which is always chat.completion"
    )
    created: int = Field(
        default_factory=lambda: int(time.time()),
        description="The Unix timestamp (in seconds) of when the chat completion was created"
    )
    model: str = Field(..., description="The model used for the chat completion")
    choices: List[ChatCompletionChoice] = Field(
        ..., description="A list of chat completion choices"
    )
    usage: Usage = Field(..., description="Usage statistics for the completion request")
    service_tier: Optional[str] = Field(
        "default", description="The service tier used for processing the request"
    )


# Streaming models
class ChatCompletionStreamChoice(BaseModel):
    index: int = Field(..., description="The index of the choice in the list of choices")
    delta: ChatMessage = Field(..., description="A chat completion delta generated by the model")
    logprobs: Optional[Dict[str, Any]] = Field(
        None, description="Log probability information for the choice"
    )
    finish_reason: Optional[Literal["stop", "length", "content_filter", "tool_calls", "function_call"]] = Field(
        None, description="The reason the model stopped generating tokens"
    )


class ChatCompletionStreamResponse(BaseModel):
    id: str = Field(..., description="A unique identifier for the chat completion")
    object: Literal["chat.completion.chunk"] = Field(
        "chat.completion.chunk", description="The object type, which is always chat.completion.chunk"
    )
    created: int = Field(
        default_factory=lambda: int(time.time()),
        description="The Unix timestamp (in seconds) of when the chat completion was created"
    )
    model: str = Field(..., description="The model used for the chat completion")
    choices: List[ChatCompletionStreamChoice] = Field(
        ..., description="A list of chat completion choices"
    )
    usage: Optional[Usage] = Field(
        None, description="Usage statistics for the completion request (only in the last chunk)"
    )

